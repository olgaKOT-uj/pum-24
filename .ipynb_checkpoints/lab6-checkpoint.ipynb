{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad10fe87c955413f",
   "metadata": {},
   "source": [
    "# Lab 6: Visualizing the training process of neural networks. Hyperparameter tuning.\n",
    "\n",
    "In this lab, you will learn how to use [wandb](https://wandb.ai/) to visualize the training process of neural networks. We are going to build and train a feed-forward neural network for recognizing handwritten digits of the MNIST dataset. The training process will be visualized in the wandb dashboard, which will allow us to monitor the loss and accuracy of the model in real-time.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Feel free to create an account at [wandb.ai](https://wandb.ai/) before starting this lab.\n",
    "\n",
    "### A simple example of how to use wandb in a typical training loop is shown below:\n",
    "\n",
    "```python\n",
    "import wandb\n",
    "\n",
    "wandb.login() # Log in to your wandb account\n",
    "\n",
    "# Start a new run\n",
    "\n",
    "some_config = {\n",
    "    'learning_rate': 0.01,\n",
    "    'layer_1_size': 128,\n",
    "    'layer_2_size': 64,\n",
    "    'batch_size': 32\n",
    "} # This is just an example of a configuration dictionary, you can put anything you want here\n",
    "\n",
    "wandb.init(project='mnist-classifier', config=some_config) # start a new run and log parameters\n",
    "\n",
    "# Here you would prepare your data, and initialize the model, optimizer, etc.\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    ...\n",
    "    wandb.log({'loss': loss, 'accuracy': accuracy})\n",
    "    # This will send the loss and accuracy to wandb and you can visualize it in the dashboard\n",
    "\n",
    "# End of the run\n",
    "wandb.finish()\n",
    "```\n",
    "\n",
    "The most important part is the `wandb.log()` function, which sends the data to the wandb dashboard. You can log any metric you want, not just loss and accuracy. The value passed to the function must be a dictionary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c701a2dc778ba9",
   "metadata": {},
   "source": [
    "## Exercise 1: Prepare data for training a mnist classifier (2 points)\n",
    "\n",
    "Before you start training a neural network, you need to prepare the data. In this exercise, you will prepare the MNIST dataset of handwritten digits for training a classifier. You should:\n",
    "\n",
    "1. Load the MNIST dataset using from `data/mnist_train.csv` and `data/mnist_test.csv` files.\n",
    "2. Normalize the data to the range [0, 1].\n",
    "3. Encode the labels using one-hot encoding.\n",
    "4. Create a PyTorch `Dataset` object for the training and test sets.\n",
    "5. Create a PyTorch `DataLoader` object for the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73671250bc707ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "    \n",
    "        data = pd.read_csv(data_path)\n",
    "        self.labels = data.iloc[:, 0].values  \n",
    "        self.features = data.iloc[:, 1:].values.astype(np.float32) / 255.0  \n",
    "        \n",
    "      \n",
    "        self.encoder = OneHotEncoder(sparse_output=False)\n",
    "        self.labels = self.encoder.fit_transform(self.labels.reshape(-1, 1))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "train_dataset = MNISTDataset('data/mnist_train.csv')\n",
    "test_dataset = MNISTDataset('data/mnist_test.csv')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "sample_data, sample_label = next(iter(train_loader))\n",
    "print(f\"Sample data shape: {sample_data.shape}\")\n",
    "print(f\"Sample label shape: {sample_label.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5dc7e06a81f23f",
   "metadata": {},
   "source": [
    "## Exercise 2: Prepare the architecture of the neural network (2 points)\n",
    "\n",
    "In this exercise, you will prepare the architecture of the neural network. You should:\n",
    "\n",
    "1. Create a neural network class that inherits from `torch.nn.Module`.\n",
    "2. The neural network should have at least one hidden layer.\n",
    "3. Use ReLU activation functions after each but the output layer.\n",
    "4. Use a softmax activation function in the output layer to get the probabilities of each class.\n",
    "\n",
    "**Feel free to experiment with the architecture of your network** - try adding more hidden layers, changing the number of neurons in each layer, etc. You can also add a dropout layer or some other regularization technique and see if it improves the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5310cb52194df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size) \n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  \n",
    "        x = self.dropout(x)       \n",
    "        x = F.relu(self.fc2(x))  \n",
    "        x = self.fc3(x)          \n",
    "        x = F.softmax(x, dim=1)  \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2efbd440294eb",
   "metadata": {},
   "source": [
    "## *Training PyTorch models on GPU\n",
    "\n",
    "**GPUs are optimized for performing matrix operations in parallel.** Although we call them \"graphics processing units\", they are actually very powerful processors that can be used for any kind of parallel computation, including training deep neural networks. In fact, data science is one of the most common applications of GPUs today, as can be seen by the revenue of companies like NVIDIA over the past few years. NVIDIA is a monopolist in the GPU market - in 2023, the company owned 92% of the data center GPU market share. As for 31 July, the 2024 revenue of NVIDIA was 60.92 billion USD, while the total revenue of 2020 was $10.92 billion. If someone benefits from the current deep learning hype, it is certainly NVIDIA.\n",
    "\n",
    "If you happen to have an NVIDIA GPU in your computer, you can use it to train your deep learning models, as PyTorch has excellent support for CUDA, which is NVIDIA's parallel computing API. To train a model on GPU, you need to explicitly tell PyTorch to move the model and the data to the GPU. \n",
    "\n",
    "Here is an example training loop that uses the GPU:\n",
    "\n",
    "```python\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # Check if a GPU is available\n",
    "\n",
    "# Initialize the model and move it to the GPU\n",
    "model = SomeNeuralNetwork().to(device)   # Move the model to the GPU\n",
    "\n",
    "for epoch in range(100):\n",
    "    for batch in data_loader:\n",
    "        X, y = batch\n",
    "        X, y = X.to(device), y.to(device)   # Move the tensors to GPU\n",
    "        \n",
    "        y_pred = model(X)   # Perform a forward pass (on the GPU)\n",
    "        loss = criterion(y_pred, y)   # Compute the loss (still on the GPU)\n",
    "        \n",
    "        ...  # The rest of the training loop\n",
    "        \n",
    "        y_pred = y_pred.detach().cpu()   # Move the predictions back to the CPU to do anything else with them\n",
    "```\n",
    "\n",
    "Note that **the model and all the tensors it uses for computation should be moved to the GPU**. You can do this by calling the `.to(device)` method on the model and the data tensors. If you want to move the data back to the CPU (to process it further, calculate metrics, visualize), you call the `.cpu()` method on the tensor.\n",
    "\n",
    "**Doing calculations on the GPU, you should be wary of few things:**\n",
    "\n",
    "* **The GPU has a limited amount of memory**, so you should be careful not to run out of memory. A typical graphics card has a few gigabytes of memory, so you should be fine with most models and datasets. However, moving very large tensors to the GPU can cause out-of-memory errors. That's one of the reasons why we use a dataloader and process the data in batches.\n",
    "* While the GPU is much faster than the CPU for large matrix operations, **transferring data between the CPU and the GPU is slow**. Therefore, it is best to minimize the number of data transfers between the CPU and the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94574b231d68216",
   "metadata": {},
   "source": [
    "## Exercise 3: Prepare the training loop (2 points)\n",
    "\n",
    "In this exercise, you will prepare the training loop. You should:\n",
    "\n",
    "1. Initialize the neural network.\n",
    "2. Define the loss function (CrossEntropyLoss) and the optimizer (Adam).\n",
    "3. Pass a dictionary with the configuration to wandb. This dictionary should contain all the hyperparameters of our model, including the learning rate, the size of the hidden layers, batch size, etc.\n",
    "4. Train the neural network. Each epoch should consist of a training and validation phase. You should log the loss and accuracy of the training and validation sets using wandb.\n",
    "5. Open you project at [wandb.ai](https://wandb.ai/) and see how cool it is!\n",
    "\n",
    "### Saving and loading the model\n",
    "As training can take some time, it is a good idea to save the model's state dictionary (its learned weights) to a file after training. You can do this with the following code:\n",
    "\n",
    "    torch.save(vae.state_dict(), 'vae.pth')\n",
    "    \n",
    "To load the model from the file, you can use the following code:\n",
    "\n",
    "    vae.load_state_dict(torch.load('vae.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d558333aa5fa0bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "wandb: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Initialize Weights & Biases (wandb)\n",
    "wandb.init(project=\"your_project_name\")\n",
    "\n",
    "# Define hyperparameters\n",
    "config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"hidden_size\": 128,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 10,\n",
    "}\n",
    "wandb.config.update(config)\n",
    "\n",
    "# Define your model (example model, replace with yours)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = 784  # Example for an image dataset\n",
    "output_size = 10   # Example for 10-class classification\n",
    "model = SimpleNN(input_size, config[\"hidden_size\"], output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "# Dummy data loaders (replace with actual dataset)\n",
    "train_loader = DataLoader([], batch_size=config[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader([], batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0, 0, 0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_correct += (predicted == targets).sum().item()\n",
    "        train_total += targets.size(0)\n",
    "    \n",
    "    train_accuracy = train_correct / train_total\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == targets).sum().item()\n",
    "            val_total += targets.size(0)\n",
    "    \n",
    "    val_accuracy = val_correct / val_total\n",
    "    \n",
    "    # Log metrics to wandb\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss / len(train_loader),\n",
    "        \"train_accuracy\": train_accuracy,\n",
    "        \"val_loss\": val_loss / len(val_loader),\n",
    "        \"val_accuracy\": val_accuracy,\n",
    "    })\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{config['epochs']}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "wandb.finish()\n",
    "\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9320359d-db5e-4178-9116-63284bc52f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a77990efcffab975",
   "metadata": {},
   "source": [
    "## Exercise 4: Easy hyperparameter tuning with wandb (2 points)\n",
    "\n",
    "Wandb allows you to perform hyperparameter tuning by automatically creating multiple runs with different hyperparameters and logging the performance of each run. Below is a brief instruction to `wandb` hyperparameter tuning, but you are more than welcome to find more information in the [official wandb guide](https://docs.wandb.ai/guides/sweeps/).\n",
    "\n",
    "Your task is to use wandb to perform hyperparameter tuning of the neural network, trying different values of the learning rate, batch size, and the size of the hidden layers. You can use the following hyperparameters:\n",
    "\n",
    "First, we need to define a dictionary with the hyperparameters that we want to tune. For example:\n",
    "\n",
    "```python\n",
    "parameters = {\n",
    "    'learning_rate': {'values': [0.01, 0.001, 0.0001]},\n",
    "    'batch_size': {'values': [32, 64, 128]},\n",
    "    'layer_1_size': {'values': [64, 128, 256]},\n",
    "    'layer_2_size': {'values': [32, 64, 128]}\n",
    "}\n",
    "```\n",
    "\n",
    "Then we need to create a dictionary with the configuration of the run:\n",
    "\n",
    "```python\n",
    "sweep_config = {\n",
    "    'name': 'mnist-sweep',\n",
    "    'method': 'grid',   # grid search, you can also try 'random' or 'bayes'\n",
    "    'metric': {'goal': 'minimize', 'name': 'val_loss'},\n",
    "    'parameters': parameters,   # that's the dictionary with the hyperparameters\n",
    "}\n",
    "```\n",
    "\n",
    "Finally, we can use the `wandb.sweep` function to perform hyperparameter tuning:\n",
    "\n",
    "```python\n",
    "sweep_id = wandb.sweep(sweep_config, project='mnist-classifier')\n",
    "```\n",
    "\n",
    "After that, we can finally run the sweep:\n",
    "\n",
    "```python\n",
    "wandb.agent(sweep_id, function=train)\n",
    "```\n",
    "where `train` is a function that trains the model and logs the metrics to wandb. This function should take a `config` argument, which will contain the hyperparameters of the run. That is how wandb knows which hyperparameters to tune.\n",
    "\n",
    "1. Rewrite the VAE training loop into a function that takes a single dictionary `parameters` as an argument, initializes the model, optimizer, and criterion, and trains the model for a fixed number of epochs. The function should log the loss and accuracy of the training and validation sets to wandb.\n",
    "2. Create a dictionary with the hyperparameters that you want to tune.\n",
    "3. Create a sweep configuration dictionary.\n",
    "4. Run the sweep and monitor the results in the wandb dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d35f63fb35d55201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicja modelu VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, layer_1_size, layer_2_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, layer_1_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_1_size, layer_2_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(layer_2_size, layer_1_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_1_size, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Funkcja treningowa\n",
    "def train(config=None):\n",
    "    try:\n",
    "        with wandb.init(config=config, project=\"vae-mnist\"):\n",
    "            config = wandb.config  # Pobranie aktualnych hiperparametrów\n",
    "            \n",
    "            # Pobieranie danych MNIST\n",
    "            transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
    "            train_loader = DataLoader(datasets.MNIST(root='./data', train=True, download=True, transform=transform),\n",
    "                                      batch_size=config.batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(datasets.MNIST(root='./data', train=False, download=True, transform=transform),\n",
    "                                    batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "            # Inicjalizacja modelu\n",
    "            model = VAE(input_dim=28*28, layer_1_size=config.layer_1_size, layer_2_size=config.layer_2_size)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model.to(device)\n",
    "\n",
    "            # Pętla treningowa\n",
    "            for epoch in range(10):  # Możesz zmienić liczbę epok\n",
    "                model.train()\n",
    "                train_loss = 0.0\n",
    "                for images, _ in train_loader:\n",
    "                    images = images.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, images)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    train_loss += loss.item()\n",
    "\n",
    "                # Pętla walidacyjna\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for images, _ in val_loader:\n",
    "                        images = images.to(device)\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, images)\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                # Logowanie do wandb\n",
    "                wandb.log({\n",
    "                    \"train_loss\": train_loss / len(train_loader),\n",
    "                    \"val_loss\": val_loss / len(val_loader)\n",
    "                })\n",
    "\n",
    "            print(f\"Zakończono trenowanie dla konfiguracji: {config}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Błąd: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b896fac35c516b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'learning_rate': {'values': [0.01, 0.001, 0.0001]},\n",
    "    'batch_size': {'values': [32, 64, 128]},\n",
    "    'layer_1_size': {'values': [64, 128, 256]},\n",
    "    'layer_2_size': {'values': [32, 64, 128]}\n",
    "}\n",
    "\n",
    "sweep_config = {\n",
    "    'name': 'vae-sweep',\n",
    "    'method': 'bayes',  # 'grid', 'random', or 'bayes'\n",
    "    'metric': {'goal': 'minimize', 'name': 'val_loss'},  # Minimize validation loss\n",
    "    'parameters': parameters\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cda3fcc-3fce-4915-b7aa-e83468b6aade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
